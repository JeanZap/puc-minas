{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ],
      "metadata": {
        "id": "XFUkd7hYW_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ],
      "metadata": {
        "id": "xdDvnMh2Js4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ],
      "metadata": {
        "id": "5pa39nBgCpt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "plt.rc('animation', html='jshtml')"
      ],
      "metadata": {
        "id": "0NeV41LFJuLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifique se existe uma GPU disponível"
      ],
      "metadata": {
        "id": "vlYNELU9KOV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
        "              \"accelerator.\")\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")"
      ],
      "metadata": {
        "id": "4IyBbSXLKRb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gymnasium**"
      ],
      "metadata": {
        "id": "kgVynGIJJEeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n",
        "    %pip install -q -U gymnasium swig\n",
        "    %pip install -q -U gymnasium[classic_control,box2d,atari,accept-rom-license]"
      ],
      "metadata": {
        "id": "sdAr6yEmKiE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
      ],
      "metadata": {
        "id": "Ug6oDMhJKnWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envs = gym.envs.registry\n",
        "sorted(envs.keys())[:5] + [\"...\"]"
      ],
      "metadata": {
        "id": "Ab1RARjzCcBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Cart-Pole é um ambiente muito simples composto por um carrinho que pode se mover para a esquerda ou para a direita, e um poste colocado verticalmente em cima dele."
      ],
      "metadata": {
        "id": "juY_V8uNXVTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envs[\"CartPole-v1\"]"
      ],
      "metadata": {
        "id": "wW8qDEAECfnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos inicializar o ambiente chamando o método reset(). Isso retorna uma observação:"
      ],
      "metadata": {
        "id": "gE1AL0CeXk_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset(seed=42)\n",
        "obs"
      ],
      "metadata": {
        "id": "OCQBWQMbChPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As observações variam de acordo com o ambiente. Neste caso, é um array NumPy 1D composto por 4 números: a posição horizontal do carrinho, sua velocidade, o ângulo do poste (0 = vertical) e a velocidade angular."
      ],
      "metadata": {
        "id": "cGZXAQQuXotW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "id": "C4z99_63CiYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = env.render()\n",
        "img.shape  # height, width, channels (3 = Red, Green, Blue)"
      ],
      "metadata": {
        "id": "CSXEjbcALAMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_environment(env, figsize=(5, 4)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    img = env.render()\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    return img"
      ],
      "metadata": {
        "id": "LGAO4_sGDrld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DZKPbItmDteO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interagindo com o ambiente: o agente precisará selecionar uma ação de um \"espaço de ação\". Vamos ver como é o espaço de ação deste ambiente:"
      ],
      "metadata": {
        "id": "F8nuWtdyanCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "id": "SBTRkjjIDxa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apenas duas ações possíveis: acelerar para a esquerda ou para a direita.\n",
        "\n",
        "Como o poste está inclinado para a direita (obs[2] > 0), vamos acelerar o carrinho para a direita:"
      ],
      "metadata": {
        "id": "LdytQyLpaybI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action = 1  # accelerate right\n",
        "obs, reward, done, truncated, info = env.step(action)\n",
        "obs"
      ],
      "metadata": {
        "id": "8MSLTrnUDzPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que o carrinho agora está se movendo para a direita (obs[1] > 0). O poste ainda está inclinado para a direita (obs[2] > 0), mas sua velocidade angular agora é negativa (obs[3] < 0), então provavelmente será inclinado para a esquerda após a próxima etapa."
      ],
      "metadata": {
        "id": "1KjnOl3dbY0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extra code – displays the environment\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2WqjL_CpD1dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O ambiente também informa ao agente quanta recompensa ele recebeu durante a última etapa:"
      ],
      "metadata": {
        "id": "5n4vsJDaboME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "id": "jc-TwkiED4ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando o jogo acaba, o ambiente retorna done=True:"
      ],
      "metadata": {
        "id": "0Vu8lqXlbt6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "done"
      ],
      "metadata": {
        "id": "U8xWDoaLD6VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alguns wrappers de ambiente podem querer interromper o ambiente antecipadamente. Por exemplo, quando um limite de tempo é atingido ou quando um objeto sai dos limites. Neste caso, truncated será definido como True."
      ],
      "metadata": {
        "id": "L5aXz5q5LaoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "truncated"
      ],
      "metadata": {
        "id": "0lTl0Ve5LZyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "info é um dicionário específico do ambiente que pode fornecer algumas informações extras que podem ser úteis para depuração ou treinamento. Por exemplo, em alguns jogos pode indicar quantas vidas o agente tem."
      ],
      "metadata": {
        "id": "7oU4xML-byXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "id": "0Nk7XcC2D8iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sequência de passos entre o momento em que o ambiente é reiniciado até que seja concluído é chamado de \"episódio\". No final de um episódio (ou seja, quando step() retorna done=True), você deve redefinir o ambiente antes de continuar a usá-lo."
      ],
      "metadata": {
        "id": "-HxykJ1ub342"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if done or truncated:\n",
        "    obs, info = env.reset()"
      ],
      "metadata": {
        "id": "xDprmzFKD9Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, como podemos fazer o poste permanecer na posição vertical? Precisaremos definir uma política para isso. Essa é a estratégia que o agente usará para selecionar uma ação em cada etapa. Ele pode usar todas as ações e observações passadas para decidir o que fazer."
      ],
      "metadata": {
        "id": "cPAS5zc6cFby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Um exemplo de uma policy**"
      ],
      "metadata": {
        "id": "vKQRQ7Y8cQ4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma estratégia simples: se o poste estiver inclinado para a esquerda, empurre o carrinho para a esquerda e vice-versa."
      ],
      "metadata": {
        "id": "k8ZLYUQ2uRZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_policy(obs):\n",
        "    angle = obs[2]\n",
        "    return 0 if angle < 0 else 1\n",
        "\n",
        "totals = []\n",
        "for episode in range(500):\n",
        "    episode_rewards = 0\n",
        "    obs, info = env.reset(seed=episode)\n",
        "    for step in range(200):\n",
        "        action = basic_policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        episode_rewards += reward\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    totals.append(episode_rewards)"
      ],
      "metadata": {
        "id": "BMqzaBx9uT4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.mean(totals), np.std(totals), min(totals), max(totals)"
      ],
      "metadata": {
        "id": "JZd-yeXWv77d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estratégia é um pouco básica demais: o melhor que ela fez foi manter o poste por apenas 63 passos. O cenário de vitória é quando o agente mantém o poste ativo por 200 passos.\n",
        "\n",
        "Visualizando 1 episódio"
      ],
      "metadata": {
        "id": "S0AhwSDdujxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = matplotlib.animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim\n",
        "\n",
        "def show_one_episode(policy, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    np.random.seed(seed)\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render())\n",
        "        action = policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        if done or truncated:\n",
        "            break\n",
        "    env.close()\n",
        "    return plot_animation(frames)\n",
        "\n",
        "show_one_episode(basic_policy)"
      ],
      "metadata": {
        "id": "k3iS3AIFu6wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sistema é instável e depois de apenas algumas oscilações, o poste acaba muito inclinado: game over."
      ],
      "metadata": {
        "id": "rgWiXRQTuUaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network Policies**"
      ],
      "metadata": {
        "id": "WglY9-p_zWP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para escolher uma ação, a rede estimará uma probabilidade para cada ação. Selecionaremos uma ação aleatoriamente de acordo com as probabilidades estimadas.\n",
        "\n",
        "existem apenas duas ações possíveis (esquerda ou direita), então precisamos apenas de um neurônio de saída: ele produzirá a probabilidade “p” da ação 0 (esquerda) e a probabilidade de ação 1 (direita) será 1 - p.\n"
      ],
      "metadata": {
        "id": "jaML4l3Qzbty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on the CPU\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "id": "V68plIWz0YCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui nosso problema é o mais simples possível: a observação atual é livre de ruído e contém o estado completo do ambiente (não precisamos analisar a ação passada para inferir a velocidade por exemplo).\n",
        "\n",
        "Vamos escrever uma pequena função de política que usará a rede neural para obter a probabilidade de movimento para a esquerda e, em seguida, vamos usá-la para executar um episódio:\n",
        "\n"
      ],
      "metadata": {
        "id": "70OiAKNq0pP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pg_policy(obs):\n",
        "    left_proba = model.predict(obs[np.newaxis], verbose=0)[0][0]\n",
        "    return int(np.random.rand() > left_proba)\n",
        "\n",
        "np.random.seed(42)\n",
        "show_one_episode(pg_policy)"
      ],
      "metadata": {
        "id": "iLmuUL-QSjhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que o resultado não é nada bom. Primeiro vamos ver se ele é capaz de aprender a política básica que usamos anteriormente: vá para a esquerda se o poste estiver inclinando para a esquerda e vá para a direita se estiver inclinando para a direita."
      ],
      "metadata": {
        "id": "bcDnsCBL3NHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradiente de Política**"
      ],
      "metadata": {
        "id": "8FrgeWr8WOrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo Policy Gradients reproduz vários episódios e, em seguida, tornando as ações em bons episódios um pouco mais prováveis, enquanto as ações em episódios ruins são um pouco menos prováveis.\n",
        "\n",
        "Vamos começar criando uma função para executar uma única etapa usando o modelo. Também vamos fazer de conta por enquanto que qualquer ação tomada é a correta, para que possamos calcular a perda e seus gradientes (vamos apenas salvar esses gradientes por enquanto e modificá-los mais tarde, dependendo de quão boa ou ruim a ação acabou ser):"
      ],
      "metadata": {
        "id": "kLFEOknJWhim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "    with tf.GradientTape() as tape:\n",
        "        left_proba = model(obs[np.newaxis])\n",
        "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
        "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    obs, reward, done, truncated, info = env.step(int(action))\n",
        "    return obs, reward, done, truncated, grads"
      ],
      "metadata": {
        "id": "yrq5WZ1eWzNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se left_proba for alto, então action provavelmente será False (já que um número aleatório entre 0 e 1 provavelmente não será maior que left_proba). E False significa 0, então y_target seria igual a 1 - 0 = 1. Em outras palavras, definimos o alvo como 1, o que significa que fingimos que a probabilidade de ir para a esquerda deveria ter sido 100% (então tomamos a ação certa).\n",
        "\n",
        "Agora vamos criar outra função que contará com a função play_one_step() para reproduzir vários episódios, retornando todas as recompensas e gradientes, para cada episódio e cada etapa:"
      ],
      "metadata": {
        "id": "CFeYHBhFW6PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "    all_rewards = []\n",
        "    all_grads = []\n",
        "    for episode in range(n_episodes):\n",
        "        current_rewards = []\n",
        "        current_grads = []\n",
        "        obs, info = env.reset()\n",
        "        for step in range(n_max_steps):\n",
        "            obs, reward, done, truncated, grads = play_one_step(\n",
        "                env, obs, model, loss_fn)\n",
        "            current_rewards.append(reward)\n",
        "            current_grads.append(grads)\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        all_rewards.append(current_rewards)\n",
        "        all_grads.append(current_grads)\n",
        "\n",
        "    return all_rewards, all_grads"
      ],
      "metadata": {
        "id": "lF3yRjeTXXlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo Policy Gradients usa o modelo para reproduzir o episódio várias vezes, depois volta e analisa todas as recompensas, as desconta e as normaliza. Então, vamos criar duas funções para isso: a primeira calculará as recompensas com desconto; a segunda normalizará as recompensas com desconto em muitos episódios."
      ],
      "metadata": {
        "id": "LRA-kESJXeka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discount_rewards(rewards, discount_factor):\n",
        "    discounted = np.array(rewards)\n",
        "    for step in range(len(rewards) - 2, -1, -1):\n",
        "        discounted[step] += discounted[step + 1] * discount_factor\n",
        "    return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
        "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
        "                              for rewards in all_rewards]\n",
        "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "    reward_mean = flat_rewards.mean()\n",
        "    reward_std = flat_rewards.std()\n",
        "    return [(discounted_rewards - reward_mean) / reward_std\n",
        "            for discounted_rewards in all_discounted_rewards]"
      ],
      "metadata": {
        "id": "uXQ0q3xiXmVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Digamos que houvesse 3 ações e, após cada ação, houvesse uma recompensa: primeiro 10, depois 0, depois -50. Se usarmos um fator de desconto de 80%, a 3ª ação receberá -50 (crédito total para a última recompensa), mas a 2ª ação receberá apenas -40 (crédito de 80% para a última recompensa) e a 1ª ação receberá 80% de -40 (-32) mais crédito total para a primeira recompensa (+10), o que leva a uma recompensa com desconto de -22:"
      ],
      "metadata": {
        "id": "25n_rFSeXquy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discount_rewards([10, 0, -50], discount_factor=0.8)"
      ],
      "metadata": {
        "id": "2hwzU9KUX30p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para normalizar todas as recompensas com desconto em todos os episódios, calculamos a média e o desvio padrão de todas as recompensas com desconto, subtraímos a média de cada recompensa com desconto e dividimos pelo desvio padrão:"
      ],
      "metadata": {
        "id": "Q-8jMN3kX7is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
        "                               discount_factor=0.8)"
      ],
      "metadata": {
        "id": "auxmOkgKX-he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treinando o modelo**"
      ],
      "metadata": {
        "id": "GCSC0jzKJfDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_factor = 0.95"
      ],
      "metadata": {
        "id": "Q1TmspqvYBWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "obs, info = env.reset(seed=42)"
      ],
      "metadata": {
        "id": "CB-Li3goTo4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.binary_crossentropy"
      ],
      "metadata": {
        "id": "grh2d77HYCj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cada iteração de treinamento, o loop abaixo chama a função play_multiple_episodes(), que reproduz o jogo 10 vezes e retorna todas as recompensas e gradientes para cada episódio e etapa.\n",
        "\n",
        "Então chamamos o discount_and_normalize_rewards() para calcular a vantagem normalizada de cada ação (final_reward). Isso fornece uma medida de quão boa ou ruim cada ação realmente foi, em retrospectiva.\n",
        "\n",
        "Em seguida, passamos por cada variável treinável e, para cada uma delas, calculamos a média ponderada dos gradientes dessa variável em todos os episódios e todas as etapas, ponderada pelo final_reward.\n",
        "\n",
        "Por fim, aplicamos esses gradientes médios usando o otimizador: as variáveis ​​treináveis ​​do modelo serão ajustadas.\n",
        "\n"
      ],
      "metadata": {
        "id": "b-IfOzTEEtNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(\n",
        "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "\n",
        "    # extra code – displays some debug info during training\n",
        "    total_rewards = sum(map(sum, all_rewards))\n",
        "    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\n",
        "          f\" mean rewards: {total_rewards / n_episodes_per_update:.1f}\", end=\"\")\n",
        "\n",
        "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
        "                                                       discount_factor)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "        mean_grads = tf.reduce_mean(\n",
        "            [final_reward * all_grads[episode_index][step][var_index]\n",
        "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
        "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "        all_mean_grads.append(mean_grads)\n",
        "\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "4n_0jC0yYF6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "show_one_episode(pg_policy)"
      ],
      "metadata": {
        "id": "Ta85iOFjYLBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo de gradientes de política simples resolveu a tarefa CartPole, mas não seria bem dimensionado para tarefas maiores e mais complexas.\n",
        "\n",
        "De fato, é ineficiente em termos de amostra, o que significa que precisa explorar o jogo por muito tempo antes que possa fazer progressos significativos. Isso se deve ao fato de que ele deve executar vários episódios para estimar a vantagem de cada ação."
      ],
      "metadata": {
        "id": "er4qJeAhHSLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referência**\n",
        "\n",
        "[Hands-on ML with Scikit-Learn, Keras & TF by Aurelien Geron](https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb)"
      ],
      "metadata": {
        "id": "IK3K4I9YF9FM"
      }
    }
  ]
}