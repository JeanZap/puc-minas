{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55df39e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\jean_\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\jean_\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk==3.8.1) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\jean_\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk==3.8.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jean_\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk==3.8.1) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jean_\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk==3.8.1) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jean_\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk==3.8.1) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement unicode==1.3.8 (from versions: 2.6, 2.7, 2.8, 2.9)\n",
      "ERROR: No matching distribution found for unicode==1.3.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.2.2\n",
      "  Using cached scikit-learn-1.2.2.tar.gz (7.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      Partial import of sklearn during the build process.\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\jean_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\jean_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\jean_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 175, in prepare_metadata_for_build_wheel\n",
      "          return hook(metadata_directory, config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-build-env-ptq3txdx\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 374, in prepare_metadata_for_build_wheel\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-build-env-ptq3txdx\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 512, in run_setup\n",
      "          super().run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-build-env-ptq3txdx\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 669, in <module>\n",
      "        File \"<string>\", line 663, in setup_package\n",
      "        File \"<string>\", line 597, in configure_extension_modules\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-install-vmyt35fi\\scikit-learn_6e6a503d6eae401d92e841ce4b15ae8c\\sklearn\\_build_utils\\__init__.py\", line 47, in cythonize_extensions\n",
      "          basic_check_build()\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-install-vmyt35fi\\scikit-learn_6e6a503d6eae401d92e841ce4b15ae8c\\sklearn\\_build_utils\\pre_build_helpers.py\", line 82, in basic_check_build\n",
      "          compile_test_program(code)\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-install-vmyt35fi\\scikit-learn_6e6a503d6eae401d92e841ce4b15ae8c\\sklearn\\_build_utils\\pre_build_helpers.py\", line 38, in compile_test_program\n",
      "          ccompiler.compile(\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-build-env-ptq3txdx\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\compilers\\C\\msvc.py\", line 384, in compile\n",
      "          self.initialize()\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-build-env-ptq3txdx\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\compilers\\C\\msvc.py\", line 294, in initialize\n",
      "          vc_env = _get_vc_env(plat_spec)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\jean_\\AppData\\Local\\Temp\\pip-build-env-ptq3txdx\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\compilers\\C\\msvc.py\", line 155, in _get_vc_env\n",
      "          raise DistutilsPlatformError(\n",
      "      distutils.errors.DistutilsPlatformError: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk==3.8.1\n",
    "%pip install unicode==1.3.8\n",
    "%pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fefee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jean_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jean_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import  sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05195f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos =\\\n",
    "[\n",
    "  # No Meio do Caminho - Carlos Drumond de Andrade\n",
    "  \"No meio do caminho tinha uma pedra\\n\"\\\n",
    "  \"tinha uma pedra no meio do caminho\\n\"\\\n",
    "  \"tinha uma pedra\\n\"\\\n",
    "  \"no meio do caminho tinha uma pedra.\\n\"\\\n",
    "  \"Nunca me esquecerei desse acontecimento\\n\"\\\n",
    "  \"na vida de minhas retinas tão fatigadas.\\n\"\\\n",
    "  \"Nunca me esquecerei que no meio do caminho\\n\"\\\n",
    "  \"tinha uma pedra\\n\"\\\n",
    "  \"tinha uma pedra no meio do caminho\\n\"\\\n",
    "  \"no meio do caminho tinha uma pedra.\"\n",
    "  ,\n",
    "  # Quadrilha - Carlos Drumond de Andrade\n",
    "  \"João amava Teresa que amava Raimundo\\n\"\\\n",
    "  \"que amava Maria que amava Joaquim que amava Lili,\\n\"\\\n",
    "  \"que não amava ninguém.\\n\"\\\n",
    "  \"João foi para os Estados Unidos, Teresa para o convento,\\n\"\\\n",
    "  \"Raimundo morreu de desastre, Maria ficou para tia,\\n\"\\\n",
    "  \"Joaquim suicidou-se e Lili casou com J. Pinto Fernandes\\n\"\\\n",
    "  \"que não tinha entrado na história.\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50c2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No meio do caminho tinha uma pedra\\ntinha uma pedra no meio do caminho\\ntinha uma pedra\\nno meio do caminho tinha uma pedra.\\nNunca me esquecerei desse acontecimento\\nna vida de minhas retinas tão fatigadas.\\nNunca me esquecerei que no meio do caminho\\ntinha uma pedra\\ntinha uma pedra no meio do caminho\\nno meio do caminho tinha uma pedra.', 'João amava Teresa que amava Raimundo\\nque amava Maria que amava Joaquim que amava Lili,\\nque não amava ninguém.\\nJoão foi para os Estados Unidos, Teresa para o convento,\\nRaimundo morreu de desastre, Maria ficou para tia,\\nJoaquim suicidou-se e Lili casou com J. Pinto Fernandes\\nque não tinha entrado na história.']\n"
     ]
    }
   ],
   "source": [
    "print(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87fcc4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processa_texto(texto):\n",
    "    padrao = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
    "    tokens_preprocessados = re.findall(padrao, texto)\n",
    "\n",
    "    tokens_preprocessados = [token.lower() for token in tokens_preprocessados]\n",
    "\n",
    "    portugues_stops = stopwords.words(\"portuguese\")\n",
    "\n",
    "    tokens_preprocessados = [\n",
    "        token for token in tokens_preprocessados if token not in portugues_stops\n",
    "    ]\n",
    "\n",
    "    tokens_preprocessados = [\n",
    "        re.sub(r\"\\d+\", \"\", token) for token in tokens_preprocessados\n",
    "    ]\n",
    "\n",
    "    tokens_preprocessados = [\n",
    "        token for token in tokens_preprocessados if token not in string.punctuation\n",
    "    ]\n",
    "\n",
    "    tokens_preprocessados = [unidecode(token) for token in tokens_preprocessados]\n",
    "\n",
    "    return \" \".join(tokens_preprocessados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75dd02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_bow(documentos_preprocessados):\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "    bow_matix = vectorizer.fit_transform(documentos_preprocessados)\n",
    "\n",
    "    vocabulario = vectorizer.get_feature_names_out()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        bow_matix.T.todense(),\n",
    "        index=vocabulario,\n",
    "        columns=[f\"doc{i+1}\" for i in range(bow_matix.shape[0])],\n",
    "    )\n",
    "\n",
    "    return {\"vocabulario\": vocabulario, \"bow_dataframe\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef803a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos_preprocessados = [pre_processa_texto(poema) for poema in documentos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0ad130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acontecimento' 'amava' 'caminho' 'casou' 'convento' 'desastre' 'desse'\n",
      " 'entrado' 'esquecerei' 'estados' 'fatigadas' 'fernandes' 'ficou'\n",
      " 'historia' 'joao' 'joaquim' 'lili' 'maria' 'meio' 'morreu' 'ninguem'\n",
      " 'nunca' 'pedra' 'pinto' 'raimundo' 'retinas' 'suicidou' 'tao' 'teresa'\n",
      " 'tia' 'unidos' 'vida']\n",
      "               doc1  doc2\n",
      "acontecimento     1     0\n",
      "amava             0     1\n",
      "caminho           1     0\n",
      "casou             0     1\n",
      "convento          0     1\n",
      "desastre          0     1\n",
      "desse             1     0\n",
      "entrado           0     1\n",
      "esquecerei        1     0\n",
      "estados           0     1\n",
      "fatigadas         1     0\n",
      "fernandes         0     1\n",
      "ficou             0     1\n",
      "historia          0     1\n",
      "joao              0     1\n",
      "joaquim           0     1\n",
      "lili              0     1\n",
      "maria             0     1\n",
      "meio              1     0\n",
      "morreu            0     1\n",
      "ninguem           0     1\n",
      "nunca             1     0\n",
      "pedra             1     0\n",
      "pinto             0     1\n",
      "raimundo          0     1\n",
      "retinas           1     0\n",
      "suicidou          0     1\n",
      "tao               1     0\n",
      "teresa            0     1\n",
      "tia               0     1\n",
      "unidos            0     1\n",
      "vida              1     0\n"
     ]
    }
   ],
   "source": [
    "bow_dict = aplica_bow(documentos_preprocessados)\n",
    "\n",
    "print(bow_dict[\"vocabulario\"])\n",
    "print(bow_dict[\"bow_dataframe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f16ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplica_tfidf(documentos_preprocessados):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(documentos_preprocessados)\n",
    "\n",
    "    vocabulario = vectorizer.get_feature_names_out()\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        tfidf_matrix.T.todense(),\n",
    "        index=vocabulario,\n",
    "        columns=[f\"doc{i+1}\" for i in range(tfidf_matrix.shape[0])],\n",
    "    )\n",
    "\n",
    "    return {\"vocabulario\": vocabulario, \"tfidf_dataframe\": df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9df116c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acontecimento' 'amava' 'caminho' 'casou' 'convento' 'desastre' 'desse'\n",
      " 'entrado' 'esquecerei' 'estados' 'fatigadas' 'fernandes' 'ficou'\n",
      " 'historia' 'joao' 'joaquim' 'lili' 'maria' 'meio' 'morreu' 'ninguem'\n",
      " 'nunca' 'pedra' 'pinto' 'raimundo' 'retinas' 'suicidou' 'tao' 'teresa'\n",
      " 'tia' 'unidos' 'vida']\n",
      "                   doc1      doc2\n",
      "acontecimento  0.086066  0.000000\n",
      "amava          0.000000  0.697486\n",
      "caminho        0.516398  0.000000\n",
      "casou          0.000000  0.116248\n",
      "convento       0.000000  0.116248\n",
      "desastre       0.000000  0.116248\n",
      "desse          0.086066  0.000000\n",
      "entrado        0.000000  0.116248\n",
      "esquecerei     0.172133  0.000000\n",
      "estados        0.000000  0.116248\n",
      "fatigadas      0.086066  0.000000\n",
      "fernandes      0.000000  0.116248\n",
      "ficou          0.000000  0.116248\n",
      "historia       0.000000  0.116248\n",
      "joao           0.000000  0.232495\n",
      "joaquim        0.000000  0.232495\n",
      "lili           0.000000  0.232495\n",
      "maria          0.000000  0.232495\n",
      "meio           0.516398  0.000000\n",
      "morreu         0.000000  0.116248\n",
      "ninguem        0.000000  0.116248\n",
      "nunca          0.172133  0.000000\n",
      "pedra          0.602464  0.000000\n",
      "pinto          0.000000  0.116248\n",
      "raimundo       0.000000  0.232495\n",
      "retinas        0.086066  0.000000\n",
      "suicidou       0.000000  0.116248\n",
      "tao            0.086066  0.000000\n",
      "teresa         0.000000  0.232495\n",
      "tia            0.000000  0.116248\n",
      "unidos         0.000000  0.116248\n",
      "vida           0.086066  0.000000\n"
     ]
    }
   ],
   "source": [
    "tfidf_dict = aplica_tfidf(documentos_preprocessados)\n",
    "\n",
    "print(tfidf_dict[\"vocabulario\"])\n",
    "print(tfidf_dict[\"tfidf_dataframe\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
